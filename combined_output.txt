

==================== START OF grid.cu ====================

#include "cuda/grid.cuh"
#include "cuda/params.cuh"
#include "cuda/particles.cuh"
#include "cuda/util.cuh"
#include "types/simulationTypes.h"
#include <cuda_runtime.h>
#include <cmath>
#include <algorithm>
#include <cstdio>
#include <thrust/sort.h>
#include <thrust/device_ptr.h>
#include <thrust/execution_policy.h>

// --- Host Functions ---

__host__ void Grid_CalculateParams(GridData *grid_data, const SimulationParams *params) {
	if (!grid_data || !params) {
		fprintf(stderr, "Error: Null pointer passed to Grid_CalculateParams.\n");
		return;
	}

	grid_data->gridCellSize = params->smoothingRadius;
	if (grid_data->gridCellSize <= 1e-6f) {
		grid_data->gridCellSize = 1e-6f;
	}
	grid_data->invGridCellSize = 1.0f / grid_data->gridCellSize;

	grid_data->domainMin = params->min;

	float4 domainExtent = make_float4(
	    params->max.x - params->min.x,
	    params->max.y - params->min.y,
	    params->max.z - params->min.z,
	    params->max.w - params->min.w);

	grid_data->gridDimensions.x = std::max(1, static_cast<int>(ceilf(domainExtent.x * grid_data->invGridCellSize)));
	grid_data->gridDimensions.y = std::max(1, static_cast<int>(ceilf(domainExtent.y * grid_data->invGridCellSize)));
	grid_data->gridDimensions.z = std::max(1, static_cast<int>(ceilf(domainExtent.z * grid_data->invGridCellSize)));
	grid_data->gridDimensions.w = std::max(1, static_cast<int>(ceilf(domainExtent.w * grid_data->invGridCellSize)));

	grid_data->numGridCells =
	    static_cast<unsigned long long>(grid_data->gridDimensions.x) *
	    static_cast<unsigned long long>(grid_data->gridDimensions.y) *
	    static_cast<unsigned long long>(grid_data->gridDimensions.z) *
	    static_cast<unsigned long long>(grid_data->gridDimensions.w);

	if (grid_data->numGridCells == 0) { // Should not happen if dimensions >= 1
		fprintf(stderr, "Warning: numGridCells calculated as 0! Forcing to 1.\n");
		grid_data->numGridCells = 1;
	}
}

__host__ GridData *GridData_CreateOnHost(int numParticles, const SimulationParams *params) {
	GridData *h_grid_data = (GridData *)malloc(sizeof(GridData));
	if (!h_grid_data) {
		fprintf(stderr, "Failed to allocate host GridData struct.\n");
		exit(-1);
	}

	Grid_CalculateParams(h_grid_data, params);

	h_grid_data->particle_hashes = (unsigned long long *)malloc(numParticles * sizeof(unsigned long long));
	h_grid_data->particle_indices = (unsigned int *)malloc(numParticles * sizeof(unsigned int));
	h_grid_data->cell_starts = (unsigned int *)malloc(h_grid_data->numGridCells * sizeof(unsigned int));
	h_grid_data->cell_ends = (unsigned int *)malloc(h_grid_data->numGridCells * sizeof(unsigned int));

	return h_grid_data;
}

__host__ void GridData_FreeOnHost(GridData *grid_data) {
	if (grid_data) {
		if (grid_data->particle_hashes) {
			free(grid_data->particle_hashes);
		}
		if (grid_data->particle_indices) {
			free(grid_data->particle_indices);
		}
		if (grid_data->cell_starts) {
			free(grid_data->cell_starts);
		}
		if (grid_data->cell_ends) {
			free(grid_data->cell_ends);
		}
		free(grid_data);
	}
}

__host__ GridData *GridData_CreateOnDevice(int numParticles, const SimulationParams *params) {

	GridData *d_grid_data = nullptr;
	GridData h_grid_data;

	// Step 1: Allocate space for the main struct on the device
	CHECK_CUDA_ERROR(cudaMalloc((void **)&d_grid_data, sizeof(GridData)));

	// Step 2 & 3: Calculate parameters into the host-side struct
	Grid_CalculateParams(&h_grid_data, params);

	// Step 4: Allocate all device arrays, storing pointers in the host struct
	CHECK_CUDA_ERROR(cudaMalloc((void **)&h_grid_data.particle_hashes, numParticles * sizeof(unsigned long long)));
	CHECK_CUDA_ERROR(cudaMalloc((void **)&h_grid_data.particle_indices, numParticles * sizeof(unsigned int)));
	CHECK_CUDA_ERROR(cudaMalloc((void **)&h_grid_data.cell_starts, h_grid_data.numGridCells * sizeof(unsigned int)));
	CHECK_CUDA_ERROR(cudaMalloc((void **)&h_grid_data.cell_ends, h_grid_data.numGridCells * sizeof(unsigned int)));

	// Step 5: Initialize the device arrays directly, avoiding custom kernels.
	// Initialize cell_ends to 0 (the starting value for atomicMax).
	// Initialize cell_starts to `numParticles` (the sentinel value for atomicMin).
	if (h_grid_data.numGridCells > 0) {
		unsigned long long numGridCells = h_grid_data.numGridCells;
		int threads_per_block = 256;
		dim3 blocks((numGridCells + threads_per_block - 1) / threads_per_block);
		dim3 threads(threads_per_block);

		Grid_InitArrayKernel<<<blocks, threads>>>(h_grid_data.cell_starts, numGridCells, numParticles);
		CHECK_CUDA_ERROR(cudaGetLastError());

		CHECK_CUDA_ERROR(cudaMemset(h_grid_data.cell_ends, 0, h_grid_data.numGridCells * sizeof(unsigned int)));
	}

	// Step 6: Copy the entire, fully-configured host struct to the device
	CHECK_CUDA_ERROR(cudaMemcpy(d_grid_data, &h_grid_data, sizeof(GridData), cudaMemcpyHostToDevice));

	return d_grid_data;
}

__host__ void GridData_FreeOnDevice(GridData *grid_data) {
	if (grid_data) {
		GridData h_grid_data_ptrs;
		CHECK_CUDA_ERROR(cudaMemcpy(&h_grid_data_ptrs, grid_data, sizeof(GridData), cudaMemcpyDeviceToHost));

		CHECK_CUDA_ERROR(cudaFree(h_grid_data_ptrs.particle_hashes));
		CHECK_CUDA_ERROR(cudaFree(h_grid_data_ptrs.particle_indices));
		CHECK_CUDA_ERROR(cudaFree(h_grid_data_ptrs.cell_starts));
		CHECK_CUDA_ERROR(cudaFree(h_grid_data_ptrs.cell_ends));

		CHECK_CUDA_ERROR(cudaFree(grid_data));
	}
}

__host__ void GridData_CopyParamsToDevice(GridData *host_grid_data, GridData *device_grid_data) {
	if (!host_grid_data || !device_grid_data) {
		fprintf(stderr, "Error: Null pointer passed to GridData_CopyParamsToDevice.\n");
		return;
	}
	// Copies parameters *only*. Does not update the device array pointers if they changed.
	CHECK_CUDA_ERROR(cudaMemcpy(&device_grid_data->gridCellSize, &host_grid_data->gridCellSize, sizeof(float), cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(&device_grid_data->invGridCellSize, &host_grid_data->invGridCellSize, sizeof(float), cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(&device_grid_data->domainMin, &host_grid_data->domainMin, sizeof(float4), cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(&device_grid_data->gridDimensions, &host_grid_data->gridDimensions, sizeof(int4), cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(&device_grid_data->numGridCells, &host_grid_data->numGridCells, sizeof(unsigned long long), cudaMemcpyHostToDevice));
}

// --- Grid Building Kernel Functions ---

__global__ void Grid_CalculateHashesKernel(
    const ParticleSystem *ps,
    GridData *grid_data) {
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i >= ps->numParticles) return;

	float4 pos = ps->pos[i];

	int4 cell_coords = Grid_GetCellCoords(pos, grid_data);
	unsigned long long hash = Grid_GetHashFromCell(cell_coords, grid_data);

	grid_data->particle_hashes[i] = hash;
	grid_data->particle_indices[i] = i;
}

__global__ void Grid_FindCellBoundsKernel(
    const unsigned long long *d_sorted_particle_hashes,
    GridData *grid_data,
    int numParticles) {
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i >= numParticles) return;

	unsigned long long current_hash = d_sorted_particle_hashes[i];

	if (current_hash < grid_data->numGridCells) {
		// Use atomic operations to update cell_starts and cell_ends
		// cell_starts initialized to numParticles, cell_ends initialized to 0
		atomicMin(&grid_data->cell_starts[current_hash], i);
		atomicMax(&grid_data->cell_ends[current_hash], i + 1); // i+1 for exclusive end
	}
}

// --- Initialization Kernel ---

__global__ void Grid_InitArrayKernel(
    unsigned int *array,
    unsigned long long num_elements,
    unsigned int value) {
	unsigned long long i = (unsigned long long)blockIdx.x * blockDim.x + threadIdx.x;
	if (i < num_elements) {
		array[i] = value;
	}
}

// --- Build Grid Function ---

__host__ void Grid_Build(GridData *grid_data, const ParticleSystem *ps, int numParticles) {
	if (numParticles == 0) return;

	dim3 threads(256);
	dim3 blocks((numParticles + threads.x - 1) / threads.x);

	// 1. Calculate a hash for each particle based on its grid cell location.
	Grid_CalculateHashesKernel<<<blocks, threads>>>(ps, grid_data);
	CHECK_CUDA_ERROR(cudaGetLastError());

	// --- Sorting ---
	// We need the device pointers on the host to configure the Thrust sort.
	// The standard pattern is to copy the GridData struct (which contains the pointers)
	// from device to a temporary host struct.
	GridData h_grid_ptrs;
	CHECK_CUDA_ERROR(cudaMemcpy(&h_grid_ptrs, grid_data, sizeof(GridData), cudaMemcpyDeviceToHost));

	// 2. Sort particles by hash using Thrust.
	// This reorders particle_indices so that particles in the same cell are adjacent.
	thrust::device_ptr<unsigned long long> hashes_ptr(h_grid_ptrs.particle_hashes);
	thrust::device_ptr<unsigned int> indices_ptr(h_grid_ptrs.particle_indices);

	try {
		thrust::sort_by_key(thrust::device, hashes_ptr, hashes_ptr + numParticles, indices_ptr);
	} catch (const thrust::system_error &e) {
		fprintf(stderr, "Thrust sort_by_key failed in Grid_Build: %s\n", e.what());
		exit(-200);
	}
	CHECK_CUDA_ERROR(cudaGetLastError());

	// --- Finding Cell Bounds ---
	unsigned long long numGridCells = h_grid_ptrs.numGridCells;
	if (numGridCells > 0) {
		// 3. Reset cell start/end arrays for this frame.
		dim3 grid_blocks((numGridCells + threads.x - 1) / threads.x);

		// Use the initialization kernel from our previous fix.
		Grid_InitArrayKernel<<<grid_blocks, threads>>>(h_grid_ptrs.cell_starts, numGridCells, numParticles);

		// Standard cudaMemset is efficient for zeroing memory.
		CHECK_CUDA_ERROR(cudaMemset(h_grid_ptrs.cell_ends, 0, numGridCells * sizeof(unsigned int)));
		CHECK_CUDA_ERROR(cudaGetLastError());

		// 4. Find the start and end index for each cell in the sorted particle array.
		// NOTE: The first argument to this kernel MUST be the sorted hashes pointer.
		Grid_FindCellBoundsKernel<<<blocks, threads>>>(h_grid_ptrs.particle_hashes, grid_data, numParticles);
		CHECK_CUDA_ERROR(cudaGetLastError());
	}
}

==================== START OF grid.cuh ====================

#pragma once

#include "types/simulationTypes.h" // For GridData, ParticleSystem, SimulationParams
#include "cuda/util.cuh"           // For CHECK_CUDA_ERROR

// --- Host Functions ---

__host__ void Grid_CalculateParams(GridData *grid_data, const SimulationParams *params);
__host__ GridData *GridData_CreateOnHost(int numParticles, const SimulationParams *params);
__host__ void GridData_FreeOnHost(GridData *grid_data); // Frees host struct only

// --- Device Functions ---

__host__ GridData *GridData_CreateOnDevice(int numParticles, const SimulationParams *params);    // Allocates device memory and copies parameters
__host__ void GridData_FreeOnDevice(GridData *grid_data);                                        // Frees device memory
__host__ void GridData_CopyParamsToDevice(GridData *host_grid_data, GridData *device_grid_data); // Copies host params to device struct

// --- Utility Device Functions ---

__device__ inline int4 Grid_GetCellCoords(float4 pos, const GridData *grid_data) {
	float4 shifted_pos = make_float4(
	    pos.x - grid_data->domainMin.x,
	    pos.y - grid_data->domainMin.y,
	    pos.z - grid_data->domainMin.z,
	    pos.w - grid_data->domainMin.w);

	float4 cell_f = make_float4(
	    shifted_pos.x * grid_data->invGridCellSize,
	    shifted_pos.y * grid_data->invGridCellSize,
	    shifted_pos.z * grid_data->invGridCellSize,
	    shifted_pos.w * grid_data->invGridCellSize);

	int4 cell_i = make_int4(static_cast<int>(floorf(cell_f.x)),
	                        static_cast<int>(floorf(cell_f.y)),
	                        static_cast<int>(floorf(cell_f.z)),
	                        static_cast<int>(floorf(cell_f.w)));

	cell_i.x = max(0, min(grid_data->gridDimensions.x - 1, cell_i.x));
	cell_i.y = max(0, min(grid_data->gridDimensions.y - 1, cell_i.y));
	cell_i.z = max(0, min(grid_data->gridDimensions.z - 1, cell_i.z));
	cell_i.w = max(0, min(grid_data->gridDimensions.w - 1, cell_i.w));

	return cell_i;
}

__device__ inline unsigned long long Grid_GetHashFromCell(int4 cell_coords, const GridData *grid_data) {
	if (cell_coords.x < 0 || cell_coords.y < 0 || cell_coords.z < 0 || cell_coords.w < 0 ||
	    cell_coords.x >= grid_data->gridDimensions.x ||
	    cell_coords.y >= grid_data->gridDimensions.y ||
	    cell_coords.z >= grid_data->gridDimensions.z ||
	    cell_coords.w >= grid_data->gridDimensions.w) {
		return grid_data->numGridCells; // Sentinel for invalid cells
	}

	// Linear index (hash) based on X then Y then Z then W changing slowest
	unsigned long long Dx = static_cast<unsigned long long>(grid_data->gridDimensions.x);
	unsigned long long Dy = static_cast<unsigned long long>(grid_data->gridDimensions.y);
	unsigned long long Dz = static_cast<unsigned long long>(grid_data->gridDimensions.z);

	unsigned long long hash = static_cast<unsigned long long>(
	    static_cast<unsigned long long>(cell_coords.w) * Dx * Dy * Dz +
	    static_cast<unsigned long long>(cell_coords.z) * Dx * Dy +
	    static_cast<unsigned long long>(cell_coords.y) * Dx +
	    static_cast<unsigned long long>(cell_coords.x));

	return hash; // In range [0, numGridCells-1] for valid cells
}

__device__ inline unsigned long long Grid_GetLinearCellIndex(int4 cell_coords, const GridData *grid_data) {
	return Grid_GetHashFromCell(cell_coords, grid_data); // Same as hash for this mapping
}

__device__ inline int4 Grid_GetGridCoordsFromLinearIndex(unsigned long long linear_index, const GridData *grid_data) {
	int4 coords;
	unsigned long long Dx = static_cast<unsigned long long>(grid_data->gridDimensions.x);
	unsigned long long Dy = static_cast<unsigned long long>(grid_data->gridDimensions.y);
	unsigned long long Dz = static_cast<unsigned long long>(grid_data->gridDimensions.z);

	coords.w = linear_index / (Dx * Dy * Dz);
	unsigned long long remainder_w = linear_index % (Dx * Dy * Dz);
	coords.z = remainder_w / (Dx * Dy);
	unsigned long long remainder_z = remainder_w % (Dx * Dy);
	coords.y = remainder_z / Dx;
	coords.x = remainder_z % Dx;

	return coords;
}

// --- Grid Building Kernel Prototypes ---

__global__ void Grid_CalculateHashesKernel(
    const ParticleSystem *ps,
    GridData *grid_data);

__global__ void Grid_FindCellBoundsKernel(
    const unsigned long long *d_sorted_particle_hashes,
    GridData *grid_data,
    int numParticles);

// --- Initialization Kernels (Called from CreateOnDevice) ---

__global__ void Grid_InitArrayKernel(
    unsigned int *array,
    unsigned long long num_elements,
    unsigned int value);

// --- Build Grid Function ---

__host__ void Grid_Build(GridData *grid_data, const ParticleSystem *ps, int numParticles);

==================== START OF info.txt ====================

gtrteimo: All Future CUDA stuff here if possible

21ChiNat: NO. Everything shall be CUDA and CUDA shall be everything

gtrteimo: AMD??? Intel?? Whatever else exists?? Not everyone has a Nvidia GPU!
            We are doing cross platform so why can't we also be hardware independent.
            At least make it optional with something like a CMake variable/option at compile time or just another branch!

21ChiNat: OK. I will make it hardware independent (huge pain in the ass), after it works with CUDA. 
            As you can see this is now in the CUDA branch and is using a completely different methodology for
            storing particles. just know much will change in this branch since openGL can be run on the GPU.
            Also i deleted the info.txt in the include so i know where to expect the continuation of this discussion.
            About CMake variable/option i have no idea how to do that but i know who knows. Anyways this branch should
            compile before i think about that.


==================== START OF params.cu ====================

#include "cuda/params.cuh"
#include "cuda/util.cuh"

// --- Host Memory Functions ---

__host__ SimulationParams *SimulationParams_CreateOnHost() {
	SimulationParams *ps = (SimulationParams *)malloc(sizeof(SimulationParams));
	if (!ps) {
		fprintf(stderr, "Host memory allocation failed for SimulationParams struct.\n");
		exit(-1);
	}
	ps->smoothingRadius = SimulationParams_Default.smoothingRadius;
	ps->gasConstantK = SimulationParams_Default.gasConstantK;
	ps->restDensity = SimulationParams_Default.restDensity;
	ps->viscosityCoefficient = SimulationParams_Default.viscosityCoefficient;
	ps->surfaceTensionCoefficient = SimulationParams_Default.surfaceTensionCoefficient;
	ps->surfaceTensionThreshold = SimulationParams_Default.surfaceTensionThreshold;
	ps->gravity = SimulationParams_Default.gravity;
	ps->min = SimulationParams_Default.min;
	ps->max = SimulationParams_Default.max;
	ps->boundaryDamping = SimulationParams_Default.boundaryDamping;
	ps->wallStiffness = SimulationParams_Default.wallStiffness;
	SimulationParams_PrecomputeKernelCoefficients(*ps);
	return ps;
}

__host__ void SimulationParams_FreeOnHost(SimulationParams *ps) {
	if (ps) {
		free(ps);
	}
}

// --- Copy Host to Device Functions ---

__host__ void SimulationParams_Copy_HostToDevice(SimulationParams *ps_host, SimulationParams *ps_device) {
	CHECK_CUDA_ERROR(cudaMemcpy(ps_device, ps_host, sizeof(SimulationParams), cudaMemcpyHostToDevice));
}

// --- Device Memory Functions ---

__host__ SimulationParams *SimulationParams_CreateOnDevice() {
	SimulationParams *ps;
	CHECK_CUDA_ERROR(cudaMalloc((void **)&ps, sizeof(SimulationParams)));
	CHECK_CUDA_ERROR(cudaMemcpy(ps, &SimulationParams_Default, sizeof(SimulationParams), cudaMemcpyHostToDevice));
	return ps;
}

__host__ void SimulationParams_FreeOnDevice(SimulationParams *ps) {
	if (ps) {
		CHECK_CUDA_ERROR(cudaFree(ps));
	}
}

// --- Copy Device to Host Functions ---

__host__ void SimulationParams_Copy_DeviceToHost(SimulationParams *ps_device, SimulationParams *ps_host) {
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host, ps_device, sizeof(SimulationParams), cudaMemcpyDeviceToHost));
}

// --- Utility Functions ---

// Call this if smoothingRadius is changed after construction
__host__ void SimulationParams_PrecomputeKernelCoefficients(SimulationParams &params) {
	if (params.smoothingRadius > 1e-6f) { // Avoid division by zero
		const float PI_F = static_cast<float>(M_PI);
		float h = params.smoothingRadius;
		float h2 = h * h;
		float h3 = h2 * h;
		float h6 = h3 * h3;
		float h9 = h3 * h6;

		params.smoothingRadiusSq = h2;
		params.poly6KernelCoeff = 315.0f / (64.0f * PI_F * h9);
		params.spikyKernelGradientCoeff = -45.0f / (PI_F * h6);
		params.viscosityKernelLaplacianCoeff = 45.0f / (PI_F * h6);
		
		// --- ADDED FOR SURFACE TENSION ---
		// Gradient of Poly6 kernel
		params.poly6KernelGradientCoeff = -945.0f / (32.0f * PI_F * h9);
		// Laplacian of Poly6 kernel
		params.poly6KernelLaplacianCoeff = -945.0f / (32.0f * PI_F * h9);
	} else {
		// ... (zeroing out coefficients)
		params.poly6KernelGradientCoeff = 0.0f;
		params.poly6KernelLaplacianCoeff = 0.0f;
	}
}

==================== START OF params.cuh ====================

#pragma once

#include "cuda/util.cuh"
#include "types/simulationTypes.h"
#include <math.h>

const SimulationParams SimulationParams_Default = {
    0.02f,                              // smoothingRadius (2cm)
    20.0f,                              // gasConstantK
    1000.0f,                            // restDensity (kg/m^3, like water)
    0.05f,                              // viscosityCoefficient
    0.0728f,                            // surfaceTensionCoefficient (N/m, like water-air)
    7.0f,                               // surfaceTensionThreshold
    {0.0f, -9.81f, 0.0f, 0.0f},         // gravity (m/s^2)
    {-1.0f, -1.0f, -1.0f, -1.0f},       // min AABB
    {1.0f, 1.0f, 1.0f, 1.0f},           // max AABB
    -0.5f,                              // boundaryDamping (negative for reflection with damping)
    3000.0f,                            // wallStiffness
    0.0016f,                            // smoothingRadiusSq (h^2)
    315.5767f / M_PI *pow(0.02f, 9.0f), // poly6KernelCoeff
    -45.0f / M_PI *pow(0.02f, 6.0f),    // spikyKernelGradientCoeff
    45.0f / M_PI *pow(0.02f, 6.0f)      // viscosityKernelLaplacianCoeff
};

// --- Host Memory Functions ---

__host__ SimulationParams *SimulationParams_CreateOnHost();
__host__ void SimulationParams_FreeOnHost(SimulationParams *ps);

// --- Copy Host to Device Functions ---

__host__ void SimulationParams_Copy_HostToDevice(SimulationParams *ps_host, SimulationParams *ps_device);

// --- Device Memory Functions ---

__host__ SimulationParams *SimulationParams_CreateOnDevice();
__host__ void SimulationParams_FreeOnDevice(SimulationParams *ps);

// --- Copy Device to Host Functions ---

__host__ void SimulationParams_Copy_DeviceToHost(SimulationParams *ps_device, SimulationParams *ps_host);

// --- Utility Functions ---

__host__ void SimulationParams_PrecomputeKernelCoefficients(SimulationParams &params);

==================== START OF particles.cu ====================

#include "cuda/particles.cuh"
#include "cuda/util.cuh"
#include <stdlib.h>

// --- Utility Functions ---

void checkIfCopyPossible(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	if (!ps_host || !ps_device) {
		fprintf(stderr, "Error: Invalid ParticleSystem pointers for copy operation.\n");
		exit(-100);
	}
	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
	if (ps_host->numParticles != ps.numParticles) {
		fprintf(stderr, "Error: Particle count mismatch between host and device ParticleSystem.\n");
		exit(-101);
	}
	if (ps_host->numParticles == 0) {
		fprintf(stderr, "Error: Particle count must be greater than zero for copy operation.\n");
		exit(-102);
	}
}

// --- Host Memory Functions ---

__host__ ParticleSystem *ParticleSystem_CreateOnHost(int numParticles) {
	ParticleSystem *ps = (ParticleSystem *)malloc(sizeof(ParticleSystem));
	if (!ps) {
		fprintf(stderr, "Host memory allocation failed for ParticleSystem struct.\n");
		exit(-1);
	}

	ps->numParticles = numParticles;

	if (numParticles > 0) {
		size_t numBytes_float4 = numParticles * sizeof(float4);
		size_t numBytes_float = numParticles * sizeof(float);

		ps->pos = (float4 *)malloc(numBytes_float4);
		ps->vel = (float4 *)malloc(numBytes_float4);
		ps->force = (float4 *)malloc(numBytes_float4);
		ps->mass = (float *)malloc(numBytes_float);
		ps->density = (float *)malloc(numBytes_float);
		ps->pressure = (float *)malloc(numBytes_float);
		ps->normal = (float4 *)malloc(numBytes_float4);
		ps->color_laplacian = (float *)malloc(numBytes_float);

		if (!ps->pos || !ps->vel || !ps->force || !ps->mass ||
		    !ps->density || !ps->pressure || !ps->normal || !ps->color_laplacian) {
			fprintf(stderr, "Host memory allocation failed for particle arrays.\n");
			exit(EXIT_FAILURE);
		}
	} else {

		ps->pos = nullptr;
		ps->vel = nullptr;
		ps->force = nullptr;
		ps->mass = nullptr;
		ps->density = nullptr;
		ps->pressure = nullptr;
		ps->normal = nullptr;
		ps->color_laplacian = nullptr;
	}
	return ps;
}

__host__ void ParticleSystem_FreeOnHost(ParticleSystem *ps) {
	if (ps) {
		if (ps->pos) free(ps->pos);
		if (ps->vel) free(ps->vel);
		if (ps->force) free(ps->force);
		if (ps->mass) free(ps->mass);
		if (ps->density) free(ps->density);
		if (ps->pressure) free(ps->pressure);
		if (ps->normal) free(ps->normal);
		if (ps->color_laplacian) free(ps->color_laplacian);
		free(ps);
	}
}

// --- Copy Host to Device Functions ---

__host__ void ParticleSystem_CopyAll_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);

	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));

	size_t numBytes_float4 = ps.numParticles * sizeof(float4);
	size_t numBytes_float = ps.numParticles * sizeof(float);

	CHECK_CUDA_ERROR(cudaMemcpy(ps.pos, ps_host->pos, numBytes_float4, cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(ps.vel, ps_host->vel, numBytes_float4, cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(ps.force, ps_host->force, numBytes_float4, cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(ps.mass, ps_host->mass, numBytes_float, cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(ps.density, ps_host->density, numBytes_float, cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(ps.pressure, ps_host->pressure, numBytes_float, cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(ps.normal, ps_host->normal, numBytes_float4, cudaMemcpyHostToDevice));
	CHECK_CUDA_ERROR(cudaMemcpy(ps.color_laplacian, ps_host->color_laplacian, numBytes_float, cudaMemcpyHostToDevice));
}

__host__ void ParticleSystem_CopyPos_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	CHECK_CUDA_ERROR(cudaMemcpy(ps_device->pos, ps_host->pos, ps_host->numParticles * sizeof(float4), cudaMemcpyHostToDevice));
}

__host__ void ParticleSystem_CopyVel_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	CHECK_CUDA_ERROR(cudaMemcpy(ps_device->vel, ps_host->vel, ps_host->numParticles * sizeof(float4), cudaMemcpyHostToDevice));
}

__host__ void ParticleSystem_CopyForce_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	CHECK_CUDA_ERROR(cudaMemcpy(ps_device->force, ps_host->force, ps_host->numParticles * sizeof(float4), cudaMemcpyHostToDevice));
}

__host__ void ParticleSystem_CopyMass_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	CHECK_CUDA_ERROR(cudaMemcpy(ps_device->mass, ps_host->mass, ps_host->numParticles * sizeof(float), cudaMemcpyHostToDevice));
}

__host__ void ParticleSystem_CopyDensity_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	CHECK_CUDA_ERROR(cudaMemcpy(ps_device->density, ps_host->density, ps_host->numParticles * sizeof(float), cudaMemcpyHostToDevice));
}

__host__ void ParticleSystem_CopyPressure_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	CHECK_CUDA_ERROR(cudaMemcpy(ps_device->pressure, ps_host->pressure, ps_host->numParticles * sizeof(float), cudaMemcpyHostToDevice));
}

__host__ void ParticleSystem_CopyNormal_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	CHECK_CUDA_ERROR(cudaMemcpy(ps_device->normal, ps_host->normal, ps_host->numParticles * sizeof(float4), cudaMemcpyHostToDevice));
}

__host__ void ParticleSystem_CopyColorLaplacian_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	CHECK_CUDA_ERROR(cudaMemcpy(ps_device->color_laplacian, ps_host->color_laplacian, ps_host->numParticles * sizeof(float), cudaMemcpyHostToDevice));
}

// --- Device Memory Functions ---

__host__ ParticleSystem *ParticleSystem_CreateOnDevice(int numParticles) {
	ParticleSystem *d_ps;
	CHECK_CUDA_ERROR(cudaMalloc((void **)&d_ps, sizeof(ParticleSystem)));
	ParticleSystem *h_ps = (ParticleSystem *)malloc(sizeof(ParticleSystem));

	h_ps->numParticles = numParticles;

	if (numParticles > 0) {
		size_t numBytes_float4 = numParticles * sizeof(float4);
		size_t numBytes_float = numParticles * sizeof(float);
		CHECK_CUDA_ERROR(cudaMalloc((void **)&h_ps->pos, numBytes_float4));
		CHECK_CUDA_ERROR(cudaMalloc((void **)&h_ps->vel, numBytes_float4));
		CHECK_CUDA_ERROR(cudaMalloc((void **)&h_ps->force, numBytes_float4));
		CHECK_CUDA_ERROR(cudaMalloc((void **)&h_ps->mass, numBytes_float));
		CHECK_CUDA_ERROR(cudaMalloc((void **)&h_ps->density, numBytes_float));
		CHECK_CUDA_ERROR(cudaMalloc((void **)&h_ps->pressure, numBytes_float));
		CHECK_CUDA_ERROR(cudaMalloc((void **)&h_ps->normal, numBytes_float4));
		CHECK_CUDA_ERROR(cudaMalloc((void **)&h_ps->color_laplacian, numBytes_float));
	}
	CHECK_CUDA_ERROR(cudaMemcpy(d_ps, h_ps, sizeof(ParticleSystem), cudaMemcpyHostToDevice));
	return d_ps;
}

__host__ void ParticleSystem_FreeOnDevice(ParticleSystem *ps_device) {
	if (ps_device) {
		ParticleSystem ps;
		CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
		CHECK_CUDA_ERROR(cudaFree(ps.pos));
		CHECK_CUDA_ERROR(cudaFree(ps.vel));
		CHECK_CUDA_ERROR(cudaFree(ps.force));
		CHECK_CUDA_ERROR(cudaFree(ps.mass));
		CHECK_CUDA_ERROR(cudaFree(ps.density));
		CHECK_CUDA_ERROR(cudaFree(ps.pressure));
		CHECK_CUDA_ERROR(cudaFree(ps.normal));
		CHECK_CUDA_ERROR(cudaFree(ps.color_laplacian));
		CHECK_CUDA_ERROR(cudaFree(ps_device));
	}
}

// --- Device Accessor Functions ---

__host__ void ParticleSystem_SetNumParticlesOnDevice(ParticleSystem *ps, int numParticles) {
	CHECK_CUDA_ERROR(cudaMemcpy(&ps->numParticles, &numParticles, sizeof(int), cudaMemcpyHostToDevice));
}

__host__ unsigned int ParticleSystem_GetNumParticlesOnDevice(ParticleSystem *ps) {
	unsigned int numParticles;
	CHECK_CUDA_ERROR(cudaMemcpy(&numParticles, &ps->numParticles, sizeof(unsigned int), cudaMemcpyDeviceToHost));
	return numParticles;
}

// --- Copy Device to Host Functions ---

__host__ void ParticleSystem_CopyAll_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);

	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));

	size_t numBytes_float4 = ps.numParticles * sizeof(float4);
	size_t numBytes_float = ps.numParticles * sizeof(float);

	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->pos, ps.pos, numBytes_float4, cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->vel, ps.vel, numBytes_float4, cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->force, ps.force, numBytes_float4, cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->mass, ps.mass, numBytes_float, cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->density, ps.density, numBytes_float, cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->pressure, ps.pressure, numBytes_float, cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->normal, ps.normal, numBytes_float4, cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->color_laplacian, ps.color_laplacian, numBytes_float, cudaMemcpyDeviceToHost));
}

__host__ void ParticleSystem_CopyPos_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->pos, ps.pos, ps_host->numParticles * sizeof(float4), cudaMemcpyDeviceToHost));
}

__host__ void ParticleSystem_CopyVel_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->vel, ps.vel, ps_host->numParticles * sizeof(float4), cudaMemcpyDeviceToHost));
}

__host__ void ParticleSystem_CopyForce_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->force, ps.force, ps_host->numParticles * sizeof(float4), cudaMemcpyDeviceToHost));
}

__host__ void ParticleSystem_CopyMass_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->mass, ps.mass, ps_host->numParticles * sizeof(float), cudaMemcpyDeviceToHost));
}

__host__ void ParticleSystem_CopyDensity_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->density, ps.density, ps_host->numParticles * sizeof(float), cudaMemcpyDeviceToHost));
}

__host__ void ParticleSystem_CopyPressure_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->pressure, ps.pressure, ps_host->numParticles * sizeof(float), cudaMemcpyDeviceToHost));
}

__host__ void ParticleSystem_CopyNormal_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->normal, ps.normal, ps_host->numParticles * sizeof(float4), cudaMemcpyDeviceToHost));
}

__host__ void ParticleSystem_CopyColorLaplacian_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device) {
	checkIfCopyPossible(ps_host, ps_device);
	ParticleSystem ps;
	CHECK_CUDA_ERROR(cudaMemcpy(&ps, ps_device, sizeof(ParticleSystem), cudaMemcpyDeviceToHost));
	CHECK_CUDA_ERROR(cudaMemcpy(ps_host->color_laplacian, ps.color_laplacian, ps_host->numParticles * sizeof(float), cudaMemcpyDeviceToHost));
}


==================== START OF particles.cuh ====================

#pragma once

#include "types/simulationTypes.h"

// --- Host Memory Functions ---

__host__ ParticleSystem *ParticleSystem_CreateOnHost(int numParticles);
__host__ void ParticleSystem_FreeOnHost(ParticleSystem *ps);

// --- Copy Host to Device Functions ---

__host__ void ParticleSystem_CopyAll_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyPos_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyVel_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyForce_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyMass_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyDensity_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyPressure_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyNormal_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyColorLaplacian_HostToDevice(ParticleSystem *ps_host, ParticleSystem *ps_device);

// --- Device Memory Functions ---

__host__ ParticleSystem *ParticleSystem_CreateOnDevice(int numParticles);
__host__ void ParticleSystem_FreeOnDevice(ParticleSystem *ps);

// --- Device Accessor Functions ---

__host__ void ParticleSystem_SetNumParticlesOnDevice(ParticleSystem *ps, int numParticles);
__host__ unsigned int ParticleSystem_GetNumParticlesOnDevice(ParticleSystem *ps);

// --- Copy Device to Host Functions ---

__host__ void ParticleSystem_CopyAll_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyPos_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyVel_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyForce_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyMass_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyDensity_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyPressure_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyNormal_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device);
__host__ void ParticleSystem_CopyColorLaplacian_DeviceToHost(ParticleSystem *ps_host, ParticleSystem *ps_device);


==================== START OF simulation.cu ====================

#include "simulation.cuh"
#include <thrust/sort.h>
#include <thrust/device_ptr.h>
#include <thrust/execution_policy.h>
#include <cmath> // For M_PI, powf, etc.
#include <vector_types.h>
#include <vector_functions.h>

// --- SPH Kernel Helper Functions (Device-Only) ---
// These functions compute the values of the SPH smoothing kernels and their derivatives.
// They use the pre-computed coefficients from the SimulationParams struct for efficiency.

// Poly6 Kernel for density calculation
__device__ inline float poly6_kernel(float r_sq, const SimulationParams *params) {
	if (r_sq >= params->smoothingRadiusSq) return 0.0f;
	float term = params->smoothingRadiusSq - r_sq;
	return params->poly6KernelCoeff * term * term * term;
}

// Spiky Kernel Gradient for pressure force
__device__ inline float4 spiky_kernel_gradient(float4 r, float dist, const SimulationParams *params) {
	if (dist >= params->smoothingRadius || dist < 1e-6f) return make_float4(0.0f, 0.0f, 0.0f, 0.0f);
	float h_minus_r = params->smoothingRadius - dist;
	float coeff = params->spikyKernelGradientCoeff * h_minus_r * h_minus_r;
	return r * (coeff / dist);
}

// Viscosity Kernel Laplacian for viscosity force
__device__ inline float viscosity_kernel_laplacian(float dist, const SimulationParams *params) {
	if (dist >= params->smoothingRadius) return 0.0f;
	return params->viscosityKernelLaplacianCoeff * (params->smoothingRadius - dist);
}

// Poly6 Kernel Gradient for surface tension normal (color field gradient)
__device__ inline float4 poly6_kernel_gradient(float4 r, float r_sq, const SimulationParams *params) {
	if (r_sq >= params->smoothingRadiusSq) return make_float4(0.0f, 0.0f, 0.0f, 0.0f);
	float term = params->smoothingRadiusSq - r_sq;
	float coeff = params->poly6KernelGradientCoeff * term * term;
	return r * coeff;
}

// Poly6 Kernel Laplacian for surface tension force (color field laplacian)
__device__ inline float poly6_kernel_laplacian(float r_sq, const SimulationParams *params) {
	if (r_sq >= params->smoothingRadiusSq) return 0.0f;
	float coeff = params->poly6KernelLaplacianCoeff;
	return coeff * (params->smoothingRadiusSq - r_sq) * (3.0f * params->smoothingRadiusSq - 7.0f * r_sq);
}

// --- Simulation Lifecycle Functions ---

Simulation *Simulation_Create(int numParticles) {
	Simulation *sim = (Simulation *)malloc(sizeof(Simulation));
	if (!sim) {
		fprintf(stderr, "Failed to allocate Simulation struct on host.\n");
		return nullptr;
	}

	// Allocate host structures
	sim->host_ps = ParticleSystem_CreateOnHost(numParticles);
	sim->host_params = SimulationParams_CreateOnHost();
	sim->host_grid = GridData_CreateOnHost(numParticles, sim->host_params);

	// Allocate device structures
	sim->device_ps = ParticleSystem_CreateOnDevice(numParticles);
	sim->device_params = SimulationParams_CreateOnDevice();
	sim->device_grid = GridData_CreateOnDevice(numParticles, sim->host_params);

	return sim;
}

void Simulation_Free(Simulation *sim) {
	if (sim) {
		// Free device memory
		ParticleSystem_FreeOnDevice(sim->device_ps);
		SimulationParams_FreeOnDevice(sim->device_params);
		GridData_FreeOnDevice(sim->device_grid);

		// Free host memory
		ParticleSystem_FreeOnHost(sim->host_ps);
		SimulationParams_FreeOnHost(sim->host_params);
		GridData_FreeOnHost(sim->host_grid);

		free(sim);
	}
}

// --- Data Synchronization Functions ---

void Simulation_CopyAll_HostToDevice(Simulation *sim) {
	Simulation_CopyParticles_HostToDevice(sim);
	Simulation_CopyParameters_HostToDevice(sim);
	Simulation_CopyGrid_HostToDevice(sim);
}

void Simulation_CopyParticles_HostToDevice(Simulation *sim) {
	ParticleSystem_CopyAll_HostToDevice(sim->host_ps, sim->device_ps);
}

void Simulation_CopyParameters_HostToDevice(Simulation *sim) {
	SimulationParams_Copy_HostToDevice(sim->host_params, sim->device_params);
}

void Simulation_CopyGrid_HostToDevice(Simulation *sim) {
	// Note: Grid copy is mostly about parameters. The main arrays are device-only.
	GridData_CopyParamsToDevice(sim->host_grid, sim->device_grid);
}

void Simulation_CopyAll_DeviceToHost(Simulation *sim) {
	Simulation_CopyParticles_DeviceToHost(sim);
	Simulation_CopyParameters_DeviceToHost(sim);
	Simulation_CopyGrid_DeviceToHost(sim);
}

void Simulation_CopyParticles_DeviceToHost(Simulation *sim) {
	ParticleSystem_CopyAll_DeviceToHost(sim->host_ps, sim->device_ps);
}

void Simulation_CopyParameters_DeviceToHost(Simulation *sim) {
	SimulationParams_Copy_DeviceToHost(sim->device_params, sim->host_params);
}

void Simulation_CopyGrid_DeviceToHost(Simulation *sim) {
	// This is not typically needed, as the grid is an intermediate structure.
	// A full implementation would copy back the arrays too.
}

// --- Simulation Control Functions ---

void Simulation_Step(Simulation *h_sim, float dt) {

	unsigned int numParticles = h_sim->host_ps->numParticles;
	if (numParticles == 0) return;
	dim3 threads(256);
	dim3 blocks((numParticles + threads.x - 1) / threads.x);

	// Get pointers to device data to pass to kernels
	ParticleSystem *d_ps = h_sim->device_ps;
	SimulationParams *d_params = h_sim->device_params;
	GridData *d_grid = h_sim->device_grid;

	// 1. Update spatial grid for neighbor search
	Grid_Build(d_grid, d_ps, numParticles);

	// 2. Reset forces from previous step
	Simulation_Kernel_ResetForces<<<blocks, threads>>>(d_ps);

	// 3. Compute density and pressure
	Simulation_Kernel_ComputeDensityPressure<<<blocks, threads>>>(d_ps, d_params, d_grid);

	// 4. Compute internal forces (pressure + viscosity)
	Simulation_Kernel_ComputeInternalForces<<<blocks, threads>>>(d_ps, d_params, d_grid);

	// 5. Compute surface tension force
	Simulation_Kernel_ComputeSurfaceTension<<<blocks, threads>>>(d_ps, d_params, d_grid);

	// 6. Apply external forces (gravity) and boundary conditions
	Simulation_Kernel_ApplyExternalAndBoundaryForces<<<blocks, threads>>>(d_ps, d_params);

	// 7. Integrate particle positions and velocities
	Simulation_Kernel_IntegrateStep<<<blocks, threads>>>(d_ps, dt);

	// Check for errors after kernel launches
	CHECK_CUDA_ERROR(cudaGetLastError());
}

// --- Simulation Kernels ---

__global__ void Simulation_Kernel_ResetForces(ParticleSystem *ps) {
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i >= ps->numParticles) return;
	ps->force[i] = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
}

__global__ void Simulation_Kernel_ComputeDensityPressure(ParticleSystem *ps, const SimulationParams *params, const GridData *grid) {
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i >= ps->numParticles) return;

	float4 pos_i = ps->pos[i];
	float density = 0.0f;

	// Add self-density first to avoid issues with zero-neighbor particles
	density += ps->mass[i] * poly6_kernel(0.0f, params);

	// Get the grid cell coordinates for particle i
	int4 cell_coords_i = Grid_GetCellCoords(pos_i, grid);

	for (int dw = -1; dw <= 1; ++dw) {
		for (int dz = -1; dz <= 1; ++dz) {
			for (int dy = -1; dy <= 1; ++dy) {
				for (int dx = -1; dx <= 1; ++dx) {
					int4 neighbor_cell_coords = cell_coords_i + make_int4(dx, dy, dz, dw);
					unsigned int hash = Grid_GetHashFromCell(neighbor_cell_coords, grid);

					if (hash >= grid->numGridCells) continue;

					unsigned int start_idx = grid->cell_starts[hash];
					if (start_idx >= ps->numParticles) continue; // Skip empty cells

					unsigned int end_idx = grid->cell_ends[hash];

					// Iterate over particles in the neighboring cell
					for (unsigned int j_idx = start_idx; j_idx < end_idx; ++j_idx) {
						unsigned int j = grid->particle_indices[j_idx];
						if (i == j) continue; // Skip self-interaction

						float4 r = pos_i - ps->pos[j];
						float r_sq = dot(r, r);

						if (r_sq < params->smoothingRadiusSq) {
							// Accumulate density from this neighbor
							density += ps->mass[j] * poly6_kernel(r_sq, params);
						}
					}
				}
			}
		}
	}

	ps->density[i] = density;
	// Simplified Tait's equation of state to compute pressure
	ps->pressure[i] = params->gasConstantK * (density - params->restDensity);
	if (ps->pressure[i] < 0.0f) {
		ps->pressure[i] = 0.0f;
	}
}

__global__ void Simulation_Kernel_ComputeInternalForces(ParticleSystem *ps, const SimulationParams *params, const GridData *grid) {
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i >= ps->numParticles) return;

	float4 pos_i = ps->pos[i];
	float4 vel_i = ps->vel[i];
	float pressure_i = ps->pressure[i];
	float density_i = ps->density[i];
	float mass_i = ps->mass[i];

	if (density_i < 1e-6f) return;

	float4 pressure_force = make_float4(0.0f, 0.0f, 0.0f, 0.0f);
	float4 viscosity_force = make_float4(0.0f, 0.0f, 0.0f, 0.0f);

	int4 cell_coords_i = Grid_GetCellCoords(pos_i, grid);

	// Iterate through neighboring cells
	for (int dw = -1; dw <= 1; ++dw) {
		for (int dz = -1; dz <= 1; ++dz) {
			for (int dy = -1; dy <= 1; ++dy) {
				for (int dx = -1; dx <= 1; ++dx) {
					int4 neighbor_cell_coords = cell_coords_i + make_int4(dx, dy, dz, dw);
					unsigned int hash = Grid_GetHashFromCell(neighbor_cell_coords, grid);
					if (hash >= grid->numGridCells) continue;

					unsigned int start_idx = grid->cell_starts[hash];
					if (start_idx >= ps->numParticles) continue;

					unsigned int end_idx = grid->cell_ends[hash];

					for (unsigned int j_idx = start_idx; j_idx < end_idx; ++j_idx) {
						unsigned int j = grid->particle_indices[j_idx];
						if (i == j) continue;

						float4 r = pos_i - ps->pos[j];
						float r_sq = dot(r, r);

						if (r_sq < params->smoothingRadiusSq) {
							float dist = sqrtf(max(1.e-12f, r_sq));
							float density_j = ps->density[j];
							if (density_j < 1e-6f) continue;

							float pressure_val = (pressure_i / (density_i * density_i)) + (ps->pressure[j] / (density_j * density_j));

							pressure_force += mass_i * ps->mass[j] * pressure_val * spiky_kernel_gradient(r, dist, params);

							viscosity_force += mass_i * params->viscosityCoefficient * ps->mass[j] *
							                   ((ps->vel[j] - vel_i) / density_j) *
							                   viscosity_kernel_laplacian(dist, params);
						}
					}
				}
			}
		}
	}
	ps->force[i] += pressure_force + viscosity_force;
}

__global__ void Simulation_Kernel_ComputeSurfaceTension(ParticleSystem *ps, const SimulationParams *params, const GridData *grid) {
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i >= ps->numParticles) return;

	float4 pos_i = ps->pos[i];
	float density_i = ps->density[i];
	if (density_i < 1e-6f) return;

	float4 color_gradient = make_float4(0.0f, 0.0f, 0.0f, 0.0f); // This is the surface normal vector
	float color_laplacian = 0.0f;

	int4 cell_coords_i = Grid_GetCellCoords(pos_i, grid);

	// Iterate through neighboring cells
	for (int dw = -1; dw <= 1; ++dw) {
		for (int dz = -1; dz <= 1; ++dz) {
			for (int dy = -1; dy <= 1; ++dy) {
				for (int dx = -1; dx <= 1; ++dx) {
					int4 neighbor_cell_coords = cell_coords_i + make_int4(dx, dy, dz, dw);
					unsigned int hash = Grid_GetHashFromCell(neighbor_cell_coords, grid);
					if (hash >= grid->numGridCells) continue;

					unsigned int start_idx = grid->cell_starts[hash];
					if (start_idx >= ps->numParticles) continue;

					unsigned int end_idx = grid->cell_ends[hash];

					for (unsigned int j_idx = start_idx; j_idx < end_idx; ++j_idx) {
						unsigned int j = grid->particle_indices[j_idx];

						float4 r = pos_i - ps->pos[j];
						float r_sq = dot(r, r);

						if (r_sq < params->smoothingRadiusSq) {
							float density_j = ps->density[j];
							if (density_j < 1e-6f) continue;

							float vol_j = ps->mass[j] / density_j;

							// Accumulate color field gradient (surface normal)
							color_gradient += vol_j * poly6_kernel_gradient(r, r_sq, params);

							// Accumulate color field laplacian
							color_laplacian += vol_j * poly6_kernel_laplacian(r_sq, params);
						}
					}
				}
			}
		}
	}

	ps->normal[i] = color_gradient;
	ps->color_laplacian[i] = color_laplacian;

	float normal_mag_sq = dot(color_gradient, color_gradient);
	if (normal_mag_sq > (params->surfaceTensionThreshold * params->surfaceTensionThreshold)) {
		// Apply surface tension force (acts to minimize surface area)
		// Force is proportional to curvature (laplacian) and in the direction of the normal
		float normal_mag = sqrtf(normal_mag_sq);
		float4 st_force = -params->surfaceTensionCoefficient * color_laplacian * (color_gradient / normal_mag);
		ps->force[i] += st_force;
	}
}

__global__ void Simulation_Kernel_ApplyExternalAndBoundaryForces(ParticleSystem *ps, const SimulationParams *params) {
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i >= ps->numParticles) return;

	// Apply gravity
	ps->force[i] += ps->mass[i] * params->gravity;

	// Boundary conditions (penalty method)
	float4 pos = ps->pos[i];
	float4 vel = ps->vel[i];
	float4 force = make_float4(0.0f, 0.0f, 0.0f, 0.0f);

	// Check against min/max bounds
	if (pos.x < params->min.x) {
		force.x += params->wallStiffness * (params->min.x - pos.x);
		if (vel.x < 0) force.x += params->boundaryDamping * vel.x;
	}
	if (pos.x > params->max.x) {
		force.x += params->wallStiffness * (params->max.x - pos.x);
		if (vel.x > 0) force.x += params->boundaryDamping * vel.x;
	}
	if (pos.y < params->min.y) {
		force.y += params->wallStiffness * (params->min.y - pos.y);
		if (vel.y < 0) force.y += params->boundaryDamping * vel.y;
	}
	if (pos.y > params->max.y) {
		force.y += params->wallStiffness * (params->max.y - pos.y);
		if (vel.y > 0) force.y += params->boundaryDamping * vel.y;
	}
	if (pos.z < params->min.z) {
		force.z += params->wallStiffness * (params->min.z - pos.z);
		if (vel.z < 0) force.z += params->boundaryDamping * vel.z;
	}
	if (pos.z > params->max.z) {
		force.z += params->wallStiffness * (params->max.z - pos.z);
		if (vel.z > 0) force.z += params->boundaryDamping * vel.z;
	}
	// Note: 4th dimension boundary is disabled in main.cpp by setting min.w > max.w.
	// This logic is correct if you enable it later.
	if (pos.w < params->min.w) {
		force.w += params->wallStiffness * (params->min.w - pos.w);
		if (vel.w < 0) force.w += params->boundaryDamping * vel.w;
	}
	if (pos.w > params->max.w) {
		force.w += params->wallStiffness * (params->max.w - pos.w);
		if (vel.w > 0) force.w += params->boundaryDamping * vel.w;
	}

	ps->force[i] += force;
}

__global__ void Simulation_Kernel_IntegrateStep(ParticleSystem *ps, float dt) {
	unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i >= ps->numParticles) return;

	float mass_i = ps->mass[i];
	if (mass_i < 1e-6f) return;

	// Symplectic Euler integration

	// a = F / m
	float4 acceleration = ps->force[i] / mass_i;

	// v(t+dt) = v(t) + a(t) * dt
	ps->vel[i] += acceleration * dt;

	// x(t+dt) = x(t) + v(t+dt) * dt
	ps->pos[i] += ps->vel[i] * dt;
}

==================== START OF simulation.cuh ====================

#pragma once

#include "cuda/util.cuh"
#include "cuda/params.cuh"
#include "cuda/particles.cuh"
#include "cuda/grid.cuh"
#include "cuda/util.cuh"
#include "types/simulationTypes.h"

struct Simulation {
	ParticleSystem *device_ps = nullptr;
	SimulationParams *device_params = nullptr;
	GridData *device_grid = nullptr;

	ParticleSystem *host_ps = nullptr;
	SimulationParams *host_params = nullptr;
	GridData *host_grid = nullptr;

	Simulation *host_sim;
	Simulation *device_sim;
};

Simulation *Simulation_Create(int numParticles);

void Simulation_Free(Simulation *sim);

// Function to synchronize

void Simulation_CopyAll_HostToDevice(Simulation *sim);
void Simulation_CopyParticles_HostToDevice(Simulation *sim);
void Simulation_CopyParameters_HostToDevice(Simulation *sim);
void Simulation_CopyGrid_HostToDevice(Simulation *sim);

void Simulation_CopyAll_DeviceToHost(Simulation *sim);
void Simulation_CopyParticles_DeviceToHost(Simulation *sim);
void Simulation_CopyParameters_DeviceToHost(Simulation *sim);
void Simulation_CopyGrid_DeviceToHost(Simulation *sim);

// --- Simulation control functions ---

// Main function to advance the SPH simulation by one time step (dt).
// This function orchestrates calls to various CUDA kernels.
// - sim: Contains all necessary data, but only device data is used.
// - dt: The time step for this simulation update.
void Simulation_Step(Simulation *sim, float dt);

// --- Simulation Kernel ---

// Kernel to reset forces for all particles to zero.
__global__ void Simulation_Kernel_ResetForces(
    ParticleSystem *ps);

// Kernel to compute density and pressure for each particle.
// Uses the spatial grid for efficient neighbor lookup.
__global__ void Simulation_Kernel_ComputeDensityPressure(ParticleSystem *ps, const SimulationParams *params, const GridData *grid);

// Kernel to compute internal SPH forces (pressure, viscosity).
__global__ void Simulation_Kernel_ComputeInternalForces(ParticleSystem *ps, const SimulationParams *params, const GridData *grid);

// Kernel to compute surface tension forces (e.g., using color field method).
// This involves calculating color field gradients (normals) and Laplacians.
__global__ void Simulation_Kernel_ComputeSurfaceTension(ParticleSystem *ps, const SimulationParams *params, const GridData *grid);

// Kernel to apply external forces (e.g., gravity) and handle boundary interactions.
__global__ void Simulation_Kernel_ApplyExternalAndBoundaryForces(ParticleSystem *ps, const SimulationParams *params);

// Kernel to integrate particle positions and velocities using current forces.
// (e.g., Leapfrog, Verlet, or Symplectic Euler integration)
__global__ void Simulation_Kernel_IntegrateStep(ParticleSystem *ps, float dt);

==================== START OF temp.py ====================

import os
import shutil

# --- Configuration ---
# The name of the big file that will be created.
OUTPUT_FILENAME = "combined_output.txt" 


def combine_files_in_folder():
    """
    Finds all files in the current directory, skips the script and output file,
    and concatenates them into one big output file.
    """
    print("Starting file combination process...")
    
    # Get a list of all items (files and directories) in the current folder.
    all_items = os.listdir("./src/cuda")

    # We will collect the paths of the files we actually want to copy here.
    files_to_copy = []
    
    for filename in sorted(all_items): # Sorting makes the order predictable
            
        # If it passed all checks, it's a file we want to copy.
        files_to_copy.append(filename)

    print(f"\nFound {len(files_to_copy)} files to combine.")

    # Open the output file in 'write binary' mode ('wb').
    # This erases the file if it already exists and ensures all file types are handled.
    try:
        with open(OUTPUT_FILENAME, 'wb') as outfile:
            for filename in files_to_copy:
                print(f"  + Adding {filename}...")
                
                # Add a separator to know where one file begins and another ends.
                # We must 'encode' the text separator into bytes to write it in binary mode.
                separator = f"\n\n==================== START OF {filename} ====================\n\n".encode('utf-8')
                outfile.write(separator)
                
                # Open the source file in 'read binary' mode ('rb').
                with open("src/cuda/"+filename, 'rb') as infile:
                    # shutil.copyfileobj is efficient for copying file contents.
                    shutil.copyfileobj(infile, outfile)

            # Add a final separator to mark the end of the combined file.
            final_separator = f"\n\n==================== END OF FILE ====================\n".encode('utf-8')
            outfile.write(final_separator)

        print(f"\nSuccess! All files have been combined into '{OUTPUT_FILENAME}'.")

    except IOError as e:
        print(f"\nAn error occurred: {e}")


# This is the standard entry point for a Python script.
if __name__ == "__main__":
    combine_files_in_folder()

==================== START OF util.cu ====================

// Still empty

==================== START OF util.cuh ====================

#pragma once

#include <cuda.h>
#include <cuda_runtime.h>
#include <stdio.h>

#define M_PI 3.14159265358979323846f

// --- CUDA Error Checking Macro ---
// This macro simplifies checking for CUDA errors after API calls
#define CHECK_CUDA_ERROR(ans) { gpuAssert((ans), __FILE__, __LINE__); }
inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)
{
   if (code != cudaSuccess)
   {
      fprintf(stderr,"CUDA Error: %s %s %d\n", cudaGetErrorString(code), file, line);
      fflush(stderr);
      if (abort) exit(code);
   }
}

// --- float4 Operators ---
__device__ __host__ __forceinline__ float4 operator+(float4 a, float4 b) { return make_float4(a.x + b.x, a.y + b.y, a.z + b.z, a.w + b.w); }
__device__ __host__ __forceinline__ float4 operator-(float4 a, float4 b) { return make_float4(a.x - b.x, a.y - b.y, a.z - b.z, a.w - b.w); }
__device__ __host__ __forceinline__ void operator+=(float4 &a, float4 b) { a.x += b.x; a.y += b.y; a.z += b.z; a.w += b.w; }
__device__ __host__ __forceinline__ void operator-=(float4 &a, float4 b) { a.x -= b.x; a.y -= b.y; a.z -= b.z; a.w -= b.w; }
__device__ __host__ __forceinline__ float4 operator*(float4 a, float s) { return make_float4(a.x * s, a.y * s, a.z * s, a.w * s); }
__device__ __host__ __forceinline__ float4 operator*(float s, float4 a) { return make_float4(a.x * s, a.y * s, a.z * s, a.w * s); }
__device__ __host__ __forceinline__ float4 operator/(float4 a, float s) { float inv_s = 1.0f / s; return make_float4(a.x * inv_s, a.y * inv_s, a.z * inv_s, a.w * inv_s); }
__device__ __host__ __forceinline__ float dot(float4 a, float4 b) { return a.x * b.x + a.y * b.y + a.z * b.z + a.w * b.w; }
__device__ __host__ __forceinline__ float length(float4 v) { return sqrtf(dot(v, v)); }

// --- int4 Operators ---

__device__ __host__ __forceinline__ int4 operator+(int4 a, int4 b) { return make_int4(a.x + b.x, a.y + b.y, a.z + b.z, a.w + b.w); }
__device__ __host__ __forceinline__ int4 operator-(int4 a, int4 b) { return make_int4(a.x - b.x, a.y - b.y, a.z - b.z, a.w - b.w); }
__device__ __host__ __forceinline__ void operator+=(int4 &a, int4 b) { a.x += b.x; a.y += b.y; a.z += b.z; a.w += b.w; }
__device__ __host__ __forceinline__ void operator-=(int4 &a, int4 b) { a.x -= b.x; a.y -= b.y; a.z -= b.z; a.w -= b.w; }
__device__ __host__ __forceinline__ int4 operator*(int4 a, int s) { return make_int4(a.x * s, a.y * s, a.z * s, a.w * s); }
__device__ __host__ __forceinline__ int4 operator*(int s, int4 a) { return make_int4(a.x * s, a.y * s, a.z * s, a.w * s); }
__device__ __host__ __forceinline__ int4 operator/(int4 a, int s) { return make_int4(a.x / s, a.y / s, a.z / s, a.w / s); }
__device__ __host__ __forceinline__ int dot(int4 a, int4 b) { return a.x * b.x + a.y * b.y + a.z * b.z + a.w * b.w; }
__device__ __host__ __forceinline__ float length(int4 v) { return sqrtf(dot(v, v)); }

==================== END OF FILE ====================
